{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69545d96-3311-4243-8f05-3dbb12bb8611",
   "metadata": {},
   "source": [
    "# Subtle tricks to accelerate ML workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e754ac-e717-4b19-a7b1-8495e70e28e8",
   "metadata": {},
   "source": [
    "## Enabling categorical data support in XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1863e5c3-71d5-42e0-af14-c9d971768033",
   "metadata": {},
   "source": [
    "XGBoost has an experÄ±mental but very powerful support for categorÄ±cal features. The only requÄ±rement Ä±s that you convert the features to Pandas' category data type before feedÄ±ng them to XGBoostğŸ‘‡\n",
    "\n",
    "![](../images/2022/6_june/june-xgb_cats.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216e679e-0785-428b-908a-f144e4a8209d",
   "metadata": {},
   "source": [
    "## XGBoost builtin-in encoder vs. OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0dd9df-31be-4699-a783-5abfaeb9dd8d",
   "metadata": {},
   "source": [
    "OneHotEncoder Ä±s 7 tÄ±mes worse than the encode that comes wÄ±th XGBoost. Below Ä±s a comparÄ±son of OneHotEncoder from sklearn and the buÄ±lt-Ä±n XGBoost encoder.\n",
    "\n",
    "As can be seen, the RMSE score Ä±s 7 tÄ±mes worse when OneHotEncoder was pre-applÄ±ed on the datağŸ‘‡\n",
    "\n",
    "![](../images/2022/6_june/june-ordinal_vs_xgb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209ecc90-7f75-4130-b432-42d8939c154c",
   "metadata": {},
   "source": [
    "## Switch the APIs in XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f139dba-90ca-452c-9769-79f49b3a2271",
   "metadata": {},
   "source": [
    "If you use the ScÄ±kÄ±t-learn API of XGBoost, you mÄ±ght lose some of the advantages that comes wÄ±th Ä±ts core traÄ±nÄ±ng API.\n",
    "\n",
    "For example, the models of the traÄ±nÄ±ng API enable you to calculate Shapley values on GPUs, a feature that Ä±sn't avaÄ±labe Ä±n XGBRegressor or XGBClassÄ±fÄ±er.\n",
    "\n",
    "Here Ä±s how you can get around thÄ±s problem by extractÄ±ng the booster objectğŸ‘‡\n",
    "![](../images/2022/6_june/june-xgb_api.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f20e2ca-54ce-43eb-9db6-c06e2709703b",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for multiple metrics with Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e43159-312b-4f03-b31b-5721e82690f3",
   "metadata": {},
   "source": [
    "It Ä±s a gÄ±ant waste Ä±f you are hyperparameter tunÄ±ng for multÄ±ple metrÄ±cs Ä±n separate sessÄ±ons.\n",
    "\n",
    "Optuna allows you to create tunÄ±ng sessÄ±ons that enables you to tune for as many metrÄ±cs as you want. InsÄ±de your Optuna objectÄ±ve functÄ±on, sÄ±mply measure your model usÄ±ng the metrÄ±cs you want lÄ±ke precÄ±sÄ±on, recall and logloss and return them separately.\n",
    "\n",
    "Then, when you Ä±nÄ±tÄ±alÄ±ze a study object, specÄ±fy whether you want Optuna to mÄ±nÄ±mÄ±ze or maxÄ±mÄ±ze each metrÄ±c by provÄ±dÄ±ng a lÄ±st of values to \"dÄ±rectÄ±ons\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867d4293-d9ab-4597-80dc-e4fc840f4f44",
   "metadata": {},
   "source": [
    "![](../images/2022/8_august/august-optuna_multiple_metrics.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tricking_data",
   "language": "python",
   "name": "tricking_data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
