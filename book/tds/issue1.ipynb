{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e872ec64-1088-4054-b751-0afec5f8b35f",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515094cc-13ea-4560-a45a-c35ef982a780",
   "metadata": {},
   "source": [
    "## 1. Permutation Importance with ELI5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6d9cb7-5cf7-46d4-854d-ed6246b8667b",
   "metadata": {},
   "source": [
    "Permutation importance is one of the most reliable ways to see the important features in a model. \n",
    "\n",
    "Its advantages:\n",
    "\n",
    "1. Works on any type of model structure\n",
    "2. Easy to interpret and implement\n",
    "3. Consistent and reliable\n",
    "\n",
    "Permutation importance of a feature is defined as the change in model performance when that feature is randomly shuffled.\n",
    "\n",
    "PI is available through the eli5 package. Below are PI scores for an XGBoost Regressor modelðŸ‘‡\n",
    "\n",
    "The show_weights function displays the features that hurt the model's performance the most after being shuffled - i.e. the most important features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192faa36-8b97-4ab3-83f1-0f95c070f388",
   "metadata": {},
   "source": [
    "![](../images/2022/6_june/eli5_pi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d31c2dd-532c-47c7-b5f6-088e1bc5b460",
   "metadata": {},
   "source": [
    "## 2. ConfusionMatrix display for better confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd82938f-e044-4fb6-bc17-0a75599a62f9",
   "metadata": {},
   "source": [
    "If you want much more control over how you display your confusion matrix in Sklearn, use ConfusionMatrixDisplay class.\n",
    "\n",
    "With the class, you can control how X and Y labels look, what texts they display, the colormap of the matrix and much more.\n",
    "\n",
    "Besides, it has a from_estimator function that enables you to plot the matrix without having to generate predictions beforehand.\n",
    "\n",
    "![](../images/2022/6_june/june-matrix_display.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccc574b-10ca-4d97-9a43-0bae2afc44d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Text representation of a decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33738fc-cbae-4e43-afe8-c9083005d824",
   "metadata": {},
   "source": [
    "Sklearn allows you to print a text representation of a decision tree. Here is an exampleðŸ‘‡\n",
    "\n",
    "After taking a minute reading the output, you can easily build a prediction path for any sample in your dataset:\n",
    "\n",
    "![](../images/2022/6_june/june-text_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4793e9e4-0362-40e0-84a3-4ad7470559cf",
   "metadata": {},
   "source": [
    "## 4. Default RMSE in Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdcfaba-c45f-4cbf-be1e-fc955f3b44ad",
   "metadata": {},
   "source": [
    "I always found it strange that Room Mean Squared Error wasn't available in Sklearn given that it was such a popular metric. \n",
    "\n",
    "Later, I found that I didn't look long enough because it was available as a parameter inside mean_squared_error (squared=False)ðŸ‘‡\n",
    "\n",
    "![](../images/2022/6_june/june-rmse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add6ee2a-0472-4903-86f1-a8d75c63055b",
   "metadata": {},
   "source": [
    "## 5. Plotting decision trees in Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68367b3c-8766-4787-ac89-e0c229072156",
   "metadata": {},
   "source": [
    "Decision trees are everywhere. It has many variations with applications - CART boosted tree in XGBoost, regular and extremely random trees of Sklearn, trees of IsolationForest for outlier detection, etc.\n",
    "\n",
    "So, it is crucial that you understand how they work. One way you can do this is by visualizing them via Sklearn:\n",
    "\n",
    "![](../images/2022/6_june/june-viz_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89a772a-25d5-4453-bbf2-3b9b1cd05205",
   "metadata": {},
   "source": [
    "## 6. Rule of thumb for fit/predict/fit_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fc724c-d3e3-4770-9637-f5d2607de2c7",
   "metadata": {},
   "source": [
    "Rules of thumb to differentiate between fit/transform/fit_transform functions of Sklearn.\n",
    "\n",
    "1. All sklearn transformers (e.g. OneHotEncoder, StandardScaler) must be fitted to the training data. When the \"fit\" function is called, the transformers learn statistical properties of the features like mean, median, variance, quartiles, etc. That's why any function that has \"fit\" in the name must be called on training data first.\n",
    "\n",
    "2. The transform function behaves differently based on the estimator's purpose. It is called only after the \"fit\" function is run because most \"transform\" functions need the information learned from \"fit\". \"transform\" can be used on all sets as long as the \"fit\" function is called on training.\n",
    "\n",
    "3. \"fit_transform\" should also be used only on training data. The only difference is that it simultaneously learns and transforms the statistical properties of the training features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbf4222-eeac-4d3d-8687-f752a14fbe36",
   "metadata": {},
   "source": [
    "## 7. The difference between micro, macro, weighted averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8e93bb-61e7-4f05-8aa3-2b54c29c2b2b",
   "metadata": {},
   "source": [
    "What are the differences between micro, macro and weighted averages and why should you care?\n",
    "\n",
    "In multi-class classification problems, models often compute a metric for each class. For example, in a 3-class problem, 3 precision scores are returned. We don't care for three, we just need a single global metric. That's where averaging methods come into play.\n",
    "\n",
    "1. Macro average\n",
    "\n",
    "This is a simple arithmetic mean. For example, if precision scores are 0.7, 0.8, 0.9, macro average would be their mean - 0.8. \n",
    "\n",
    "2. Weighted average\n",
    "\n",
    "This method takes into account the class imbalance as metrics for each class are multiplied by the proportion of that class. For example, if there are 100 samples (30, 45, 25 for each class respectively) and the precision scores are .7, .8, .9, the weighted average would be:\n",
    "\n",
    "0.3 * 0.7 + 0.45 * 0.8 + 0.25 * 0.9 = 0.795\n",
    "\n",
    "3. Micro average\n",
    "\n",
    "Micro average is the same as accuracy - it is calculated by dividing the number of all correctly classified samples (true positives) by the total number of correctly and incorrectly (true positives + false positives) classified samples of each class.\n",
    "\n",
    "You should avoid micro average when you have an imbalanced problem. Instead, use macro if you don't care much for class contributions or weighted average when you do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24d4829-cbae-4238-9410-d73db252345d",
   "metadata": {},
   "source": [
    "## 8. Saving to parquet is much faster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae77cbbe-8106-4185-ab42-c2d2ffe1a788",
   "metadata": {},
   "source": [
    "Saving and loading Parquet files are much faster and painless. Here is a comparison of how much it takes to save an 11GB dataframe to Parquet and CSVðŸ‘‡\n",
    "\n",
    "![](../images/2022/6_june/june-parquet_vs_csv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7154615d-002f-47ca-afaf-6de6cfce9571",
   "metadata": {},
   "source": [
    "## 9. Parallel execution with joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfbd484-faf1-493b-9a02-1f1a99e7c2cb",
   "metadata": {},
   "source": [
    "Below is an example of how you can send a thousand HTTP requests in just 2.5 minutes with joblibðŸ‘‡\n",
    "\n",
    "joblib enables you to fully utilize the cores in your CPU by writing parallel code for your large loops. As a result, you can execute a single function in multiple threads, without wasting time and idle resources.\n",
    "\n",
    "The library accepts any picklable function, like functions for image resizing, web scraping, file operations, etc.\n",
    "\n",
    "![](../images/2022/6_june/june-joblib_parallel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44d2dda-430e-44e8-bf71-8900cc20eda0",
   "metadata": {},
   "source": [
    "## 10. Getting a scorer object from just the name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949b184d-b295-4a07-b0f5-33f8082f82b8",
   "metadata": {},
   "source": [
    "In a single project, you may evaluate your models using multiple metrics. Instead of importing them one by one from sklearn and pollute your namespace, you can use the \"get_scorer\" function of the metrics module.\n",
    "\n",
    "Just pass the name of the metric you want and you get a scorer object ready to useðŸ‘‡\n",
    "\n",
    "![](../images/2022/6_june/june-get_scorer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf95e77-f429-4eb5-a8c2-104dc69b3f97",
   "metadata": {},
   "source": [
    "## 11. Enabling categorical data support in XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584d79e0-c689-42e1-8ac8-9a8ad8c5354e",
   "metadata": {},
   "source": [
    "XGBoost has an experimental but very powerful support for categorical features. The only requirement is that you convert the features to Pandas' category data type before feeding them to XGBoostðŸ‘‡\n",
    "\n",
    "![](../images/2022/6_june/june-xgb_cats.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a01aac-c53b-461d-b7c9-cd15f4e89c31",
   "metadata": {},
   "source": [
    "## 12. Set numeric display precision in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3084952a-c123-43dd-bcb9-0aa5abca4ef3",
   "metadata": {},
   "source": [
    "It is very annoying when Pandas shows long floats in scientific notation. I usually struggle with approximating close-to-zero floats. \n",
    "\n",
    "To prevent this, you can change the display option of Pandas to limit the floating point precisionðŸ‘‡\n",
    "![](../images/2022/6_june/june-pandas_precision.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dbfe69-9309-47f3-a99c-c19498ff7491",
   "metadata": {},
   "source": [
    "## 13. XGBoost builtin-in encoder vs. OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d447ee-886a-4b25-8fcc-46a9bedc9642",
   "metadata": {},
   "source": [
    "OneHotEncoder is 7 times worse than the encode that comes with XGBoost. Below is a comparison of OneHotEncoder from sklearn and the built-in XGBoost encoder.\n",
    "\n",
    "As can be seen, the RMSE score is 7 times worse when OneHotEncoder was pre-applied on the dataðŸ‘‡\n",
    "\n",
    "![](../images/2022/6_june/june-ordinal_vs_xgb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a37e29d-45c0-40eb-8a10-9585fd897a9a",
   "metadata": {},
   "source": [
    "## 14. Get all scorer's names in Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73454d8b-933b-4856-a74c-910474a5cad7",
   "metadata": {},
   "source": [
    "Sklearn has over 50 metrics to evaluate the performance of its models. To pass those metrics inside pipelines or GridSearch instances, you have to remember their text names. \n",
    "\n",
    "If you forget any of them, here is how you can print out the names of all the metricsðŸ‘‡\n",
    "\n",
    "![](../images/2022/6_june/june-all_scorers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c940c3f-b3ef-468b-bdc5-396e98a33202",
   "metadata": {},
   "source": [
    "## 15. Best overfitting advice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaafe660-e84d-4346-9d56-d102d67b2686",
   "metadata": {},
   "source": [
    "This is the best advice I read on combatting overfitting:\n",
    "\n",
    "\"To achieve the perfect fit, you must first overfit\".\n",
    "\n",
    "Here are the reasons why:\n",
    "\n",
    "First, it makes sense - you can't fight overfitting without a model that overfits.\n",
    "\n",
    "Second, it is a sign of power - if a model is overfitting or perfectly memorizing the training data, it is a sign that model has enough optimization power to learn the patterns in the training data. \n",
    "\n",
    "Solving ML problems is all about the tension between optimization (how well the model learns from training data) and generalization (how well the model performs on unseen data). \n",
    "\n",
    "After you can build a model that is able to overfit, you should focus on generalization because too much optimization hurts it. You should try less complex model architectures, apply regularization, add random dropout layers (DART trees of XGBoost or DropOut layers in TensorFlow) to tune optimization and increase generalization.\n",
    "\n",
    "You won't be able to do any of them unless you have a model that overfits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b8c501-40b8-4034-9c50-9e98cf6f91c2",
   "metadata": {},
   "source": [
    "## 16. Generate a synthetic dataset with outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c77c30a-3778-407d-baa0-e3345090dc76",
   "metadata": {},
   "source": [
    "Anomaly detection is a fascinating unsupervised problem. To practice solving it, you can use the PyOD (Python Outlier Detection) library's generate_data function.\n",
    "\n",
    "Its features are:\n",
    "\n",
    "1. Controlling the proportion of outliers in the data (contamination)\n",
    "2. Choosing the number of informative and uninformative features\n",
    "3. Return the inlier/outlier labels if desired\n",
    "\n",
    "Here is an example 2-dimensional dataset generated with the function and visualized with Seaborn:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca331d6-2adb-4e03-ac98-7b147ca99e1e",
   "metadata": {},
   "source": [
    "![](../images/2022/6_june/june-outlier_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5644b1b7-7c5c-4e96-8b20-bfb95d839a64",
   "metadata": {},
   "source": [
    "## 17. Switch the APIs in XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5568459-73e0-4a97-aabf-fb22a05fe542",
   "metadata": {},
   "source": [
    "If you use the Scikit-learn API of XGBoost, you might lose some of the advantages that comes with its core training API.\n",
    "\n",
    "For example, the models of the training API enable you to calculate Shapley values on GPUs, a feature that isn't availabe in XGBRegressor or XGBClassifier.\n",
    "\n",
    "Here is how you can get around this problem by extracting the booster objectðŸ‘‡\n",
    "![](../images/2022/6_june/june-xgb_api.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cb7fbf-c0da-4c86-a4fa-ea3705898fc8",
   "metadata": {},
   "source": [
    "## 18. Conditionals replaced by dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa63a94-5590-4f9c-9107-c3b698e58ca8",
   "metadata": {},
   "source": [
    "You can greatly simplify your conditional statements by using dictionaries. \n",
    "\n",
    "Of course this approach has its drawbacks, but I have used it to great effect in a project where I collapsed a nested conditional block over 100 lines into just a dozen.\n",
    "\n",
    "![](../images/2022/6_june/june-conditional_dict.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d424b7e1-bf6c-4078-b746-b141ed84ba6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 19. DTreeViz package to plot decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebe9247-889a-4e55-8fcf-f9ef0ffad44a",
   "metadata": {},
   "source": [
    "Visualizing decision trees can be a very fun way of learning how they work. One of the best packages to perform this is the \"dtreeviz\" package. Here is a sample visual of a decision tree trained on the Iris dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d5e264-130c-4dde-8069-44443563b731",
   "metadata": {},
   "source": [
    "![](../images/2022/6_june/june-dtreeviz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cdc2b9-bd00-4b00-b123-10e8407707ef",
   "metadata": {},
   "source": [
    "https://mljar.com/blog/visualize-decision-tree/output_19_0.svg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0535ae4b-c4b8-4025-8bd7-cf03498b54d3",
   "metadata": {},
   "source": [
    "Credit: mljar.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111f4d55-fdc6-45ec-8db8-6ec5dbc7cf5d",
   "metadata": {},
   "source": [
    "## 20. Set displaying max number of rows and columns in pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e2bc96-3dc0-40e1-8689-f4cadc4ee2b6",
   "metadata": {},
   "source": [
    "Isn't it frustrating when Pandas clips the output of dataframes when there are too much columns or rows? You can get rid of that pesky problem by setting the display option of max number of columns and rows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53d2c10-7f19-444f-8a6c-e98109f5e08d",
   "metadata": {},
   "source": [
    "![](../images/2022/6_june/june-max_row_col.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e9cac2-e2ff-4c0d-83cd-696a00625ea6",
   "metadata": {},
   "source": [
    "## 21. Caching functions in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c21ffd-f961-4e26-b93a-9e4ebe37d2c0",
   "metadata": {},
   "source": [
    "Since version 3.9, Python has its own caching decorator in the \"functools\" module. \n",
    "\n",
    "It is dead useful when working with recursive functions or functions that work with memory-heavy arguments. Here is an example use-case from Python docs:\n",
    "\n",
    "![](../images/2022/6_june/june-cache.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b58fd8-82d3-4174-8bce-8fcab3b64e02",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cfa125-330a-47c3-a198-cd2c2d2be31c",
   "metadata": {},
   "source": [
    "## 1. Made With ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c04da9-ba0c-45a7-8bb0-0cfb305f459a",
   "metadata": {},
   "source": [
    "There are so darn many MLOps tools right now. The only thing that is more than that is the number of courses, books and resources on MLOps. \n",
    "\n",
    "To learn MLOps properly, you only need a few high-quality resources, not dozen. The open-source Made With ML website is one of them.\n",
    "\n",
    "It teaches everything you need to know to take models from idea to deployment. It has got separate chapters for\n",
    "\n",
    "âœ… Model development\n",
    "âœ… Packaging\n",
    "âœ… Deployment\n",
    "âœ… Testing\n",
    "âœ… Reproducibility\n",
    "\n",
    "The project has over +30k on GitHub, making it one of the most popular repositors in the MLOps world.\n",
    "\n",
    "Link to the website: https://madewithml.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96339272-8b5e-46f2-b7a7-934235ed525e",
   "metadata": {},
   "source": [
    "![image.png](../images/2022/6_june/madewithml.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba5366d-d1bd-4675-990b-30dbec2c0c19",
   "metadata": {},
   "source": [
    "## 2. 460 free textbooks on math, science and statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fca5ab-20b2-4d22-be28-4d15a92de18f",
   "metadata": {},
   "source": [
    "If you are one of the people who get a dopamine hit by collecting more books than they need, this list is for you.\n",
    "\n",
    "FreeCodeCamp put together a list of over 450 free texts on math, science and statistics, each a single click away: https://bit.ly/3ygDI4u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780f716f-f7f2-4916-bc0b-51b14652202a",
   "metadata": {},
   "source": [
    "## 3. Yann LeCun's Deep Learning Course at CDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d61596-bb12-4756-a769-36026c8789b7",
   "metadata": {},
   "source": [
    "A free deep learning course by Yann LeCun himself!\n",
    "\n",
    "Yann LeCun is one of the three people who are considered as \"Godfathers of AI and Deep Learning\". He has a Turing award as well, which is considered as the Nobel Prize of computing.\n",
    "\n",
    "If he is teaching a course and for free, you should definitely take it: https://cds.nyu.edu/deep-learning/\n",
    "\n",
    "The course covers:\n",
    "\n",
    "âœ… Supervised and unsupervised deep learning\n",
    "âœ… Embedding methods\n",
    "âœ… Metric learning\n",
    "âœ… Convnets and RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47ddc75-a5c0-4cf4-82c8-de59d6d05c2c",
   "metadata": {},
   "source": [
    "## 4. Awesome ML books by Awesome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e8d48a-6e54-4545-ab00-1b7ed5282f78",
   "metadata": {},
   "source": [
    "Awesome ML books by Awesome!\n",
    "\n",
    "Awesome is a GitHub project with over 200k GitHub starts that outlines a list of lists of all interesting things in programming and computing.\n",
    "\n",
    "It has a separate list for free and open-source machine learning books as well: https://bit.ly/3ynmYZw\n",
    "\n",
    "The list is awesome!\n",
    "\n",
    "![](../images/2022/6_june/awesome_books.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940271e5-d912-40bd-bf42-407cb11ba698",
   "metadata": {},
   "source": [
    "## 5. Intro to Pprobability for data science by michigan university (PDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4342575-77d4-4a70-9854-4c3dd8e7fd8c",
   "metadata": {},
   "source": [
    "Probability is the backbone of data science and machine learning. Let it penetrate your brain using this superb  free book by Michigan University: https://probability4datascience.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba17b7e1-ae18-404a-89cb-b402dd4d3d04",
   "metadata": {},
   "source": [
    "## 6. Papers with Code datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a03db8-1158-466e-8531-c6709dba86b4",
   "metadata": {},
   "source": [
    "Want to get your hands on high-quality datasets used in cutting-edge research?\n",
    "\n",
    "The Papers with Code dataset has a curated list of over 6500 such datasets with controls to filter by data type, task and language.\n",
    "\n",
    "Link to the source: https://bit.ly/3QP50pH\n",
    "\n",
    "![](../images/2022/6_june/datasets_pwc.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061af4bf-5737-47ab-beda-26aedb29cfb1",
   "metadata": {},
   "source": [
    "## 7. Writing a scientific article from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb007057-e349-485c-9a02-8b40bff5f79d",
   "metadata": {},
   "source": [
    "If you think writing a scientific article is easy and fun, you and I have very different interpretations of these words. \n",
    "\n",
    "Formal and technical writing is probably the most challenging type of writing. \n",
    "\n",
    "If done right, scientific publication can open wide doors for career advancement and academic reputaion. Learn to ace it in the guide below:\n",
    "\n",
    "Download: https://bit.ly/3u2ti5U\n",
    "\n",
    "Read: https://bit.ly/3njH0xC\n",
    "\n",
    "![image.png](../images/2022/6_june/scientific_article.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tricking_data",
   "language": "python",
   "name": "tricking_data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
